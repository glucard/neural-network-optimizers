# neural-network-optimizers
The proposition of this project was to study and implement from scratch the most commons and efficient neural network optimizers:
 - Dropout;
 - Momentum optimizer;<br/>
![Cost function](https://github.com/glucard/neural-network-optimizers/blob/main/src/predictions_mini_batch_momentum_optimizer/_cost_function_graph_0.0024936141180071604.png)
 - RMSprop;<br/>
![Cost function](https://github.com/glucard/neural-network-optimizers/blob/main/src/predictions_mini_batch_RMSprop/_cost_function_graph_0.0001370692396139821.png)
 - Adam.<br/>
![Cost function](https://github.com/glucard/neural-network-optimizers/blob/main/src/predictions_mini_batch_AdamOpitimizer/_cost_function_graph_0.00012881034701785337.png)
